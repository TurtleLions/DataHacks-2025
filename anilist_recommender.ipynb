{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: python script.py <watchlist_xml_file> <anime_dataset_csv_file> [top_n]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import ast\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import process\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import hstack, csr_matrix, vstack\n",
    "\n",
    "def parse_watchlist(xml_file):\n",
    "    \"\"\"\n",
    "    Parses the user's anime watchlist from a MyAnimeList XML export.\n",
    "    Only includes anime with a watched episodes count greater than 0.\n",
    "    Returns a list of anime titles.\n",
    "    \"\"\"\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    watchlist = []\n",
    "    for anime in root.findall('anime'):\n",
    "        watched_elem = anime.find('my_watched_episodes')\n",
    "        try:\n",
    "            watched_count = int(watched_elem.text.strip()) if watched_elem is not None and watched_elem.text else 0\n",
    "        except ValueError:\n",
    "            watched_count = 0\n",
    "        # Only include anime if watched episodes > 0\n",
    "        if watched_count > 0:\n",
    "            title_elem = anime.find('series_title')\n",
    "            if title_elem is not None and title_elem.text:\n",
    "                watchlist.append(title_elem.text.strip())\n",
    "    return watchlist\n",
    "\n",
    "def load_anime_dataset(csv_file):\n",
    "    \"\"\"\n",
    "    Loads the Kaggle anime dataset from a CSV file.\n",
    "    Fills missing values for required columns.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    required_columns = ['genres', 'averageScore', 'popularity', 'description', 'tags',\n",
    "                        'title_romaji', 'title_english', 'title_native']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "        # Replace NaN values with empty strings\n",
    "        df[col] = df[col].fillna('')\n",
    "    return df\n",
    "\n",
    "def match_watchlist_titles(watchlist, df, threshold=70):\n",
    "    \"\"\"\n",
    "    Uses fuzzy matching to align watchlist titles with the dataset.\n",
    "    Checks against title_romaji, title_english, and title_native fields.\n",
    "    Returns a dict mapping watchlist titles to the matched dataset index.\n",
    "    If no match is found above the threshold, a warning is printed.\n",
    "    \"\"\"\n",
    "    matched_indices = {}\n",
    "    for title in watchlist:\n",
    "        best_score = -1\n",
    "        best_idx = None\n",
    "        for idx, row in df.iterrows():\n",
    "            candidate_titles = [str(row['title_romaji']), str(row['title_english']), str(row['title_native'])]\n",
    "            for candidate in candidate_titles:\n",
    "                if candidate.strip() == '':\n",
    "                    continue\n",
    "                _, score = process.extractOne(title, [candidate])\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_idx = idx\n",
    "        if best_score < threshold:\n",
    "            print(f\"Warning: No good match found for title '{title}'.\")\n",
    "        else:\n",
    "            matched_indices[title] = best_idx\n",
    "    return matched_indices\n",
    "\n",
    "def process_genres(df):\n",
    "    \"\"\"\n",
    "    Processes the 'genres' column.\n",
    "    Converts string representation of lists into Python lists,\n",
    "    then one-hot encodes using MultiLabelBinarizer.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import MultiLabelBinarizer\n",
    "    genres_list = []\n",
    "    for genres_str in df['genres']:\n",
    "        try:\n",
    "            genres = ast.literal_eval(genres_str)\n",
    "            if not isinstance(genres, list):\n",
    "                genres = []\n",
    "        except Exception:\n",
    "            genres = []\n",
    "        genres_list.append(genres)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genres_encoded = mlb.fit_transform(genres_list)\n",
    "    return genres_encoded, mlb\n",
    "\n",
    "def process_tags(df):\n",
    "    \"\"\"\n",
    "    Processes the 'tags' column.\n",
    "    Parses the column (which is a string representing a list of dictionaries),\n",
    "    extracts tag names and converts their ranks into a percentage (rank/100),\n",
    "    then encodes them into a vector for each anime based on all tags found in the dataset.\n",
    "    \"\"\"\n",
    "    tags_dicts = []\n",
    "    for tags_str in df['tags']:\n",
    "        try:\n",
    "            tags_data = ast.literal_eval(tags_str)\n",
    "        except Exception:\n",
    "            tags_data = []\n",
    "        tag_dict = {}\n",
    "        if isinstance(tags_data, list):\n",
    "            for tag in tags_data:\n",
    "                if isinstance(tag, dict) and 'name' in tag and 'rank' in tag:\n",
    "                    try:\n",
    "                        tag_percentage = float(tag['rank']) / 100.0\n",
    "                    except Exception:\n",
    "                        tag_percentage = 0\n",
    "                    tag_dict[tag['name']] = tag_percentage\n",
    "        tags_dicts.append(tag_dict)\n",
    "    \n",
    "    # Build a set of all tag names across the dataset\n",
    "    all_tags = set()\n",
    "    for d in tags_dicts:\n",
    "        for tag in d.keys():\n",
    "            all_tags.add(tag)\n",
    "    all_tags = sorted(list(all_tags))  # Sorted for consistency\n",
    "    \n",
    "    # Encode each anime's tags into a vector with weighted percentages\n",
    "    tags_vectors = []\n",
    "    for d in tags_dicts:\n",
    "        vector = [d.get(tag, 0) for tag in all_tags]\n",
    "        tags_vectors.append(vector)\n",
    "    \n",
    "    tags_encoded = np.array(tags_vectors)\n",
    "    return tags_encoded, all_tags\n",
    "\n",
    "def process_description(df):\n",
    "    \"\"\"\n",
    "    Processes the anime description using TF-IDF vectorization.\n",
    "    Returns the normalized TF-IDF matrix.\n",
    "    \"\"\"\n",
    "    descriptions = df['description'].fillna('').tolist()\n",
    "    tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "    tfidf_matrix = tfidf.fit_transform(descriptions)\n",
    "    tfidf_matrix = normalize(tfidf_matrix)\n",
    "    return tfidf_matrix, tfidf\n",
    "\n",
    "def process_numeric_feature(df, column_name):\n",
    "    \"\"\"\n",
    "    Processes a numeric column (e.g., averageScore or popularity)\n",
    "    using MinMax scaling. Returns the scaled column.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    values = df[[column_name]].astype(float).fillna(0)\n",
    "    scaled = scaler.fit_transform(values)\n",
    "    return scaled, scaler\n",
    "\n",
    "def combine_features(tfidf_matrix, genres_encoded, tags_encoded, score_scaled, popularity_scaled,\n",
    "                     weights={'synopsis': 0.4, 'genres': 0.2, 'tags': 0.2, 'score': 0.1, 'popularity': 0.1}):\n",
    "    \"\"\"\n",
    "    Combines the different features into a single vector for each anime.\n",
    "    Each feature is first normalized and weighted.\n",
    "    The resulting vectors are horizontally stacked and then normalized.\n",
    "    \"\"\"\n",
    "    genres_norm = normalize(genres_encoded, norm='l2', axis=1)\n",
    "    tags_norm = normalize(tags_encoded, norm='l2', axis=1) if tags_encoded.shape[1] > 0 else tags_encoded\n",
    "    score_norm = normalize(score_scaled, norm='l2', axis=0)\n",
    "    popularity_norm = normalize(popularity_scaled, norm='l2', axis=0)\n",
    "    \n",
    "    synopsis_weighted = tfidf_matrix * weights['synopsis']\n",
    "    genres_weighted = genres_norm * weights['genres']\n",
    "    tags_weighted = tags_norm * weights['tags']\n",
    "    score_weighted = score_norm * weights['score']\n",
    "    popularity_weighted = popularity_norm * weights['popularity']\n",
    "    \n",
    "    genres_sparse = csr_matrix(genres_weighted)\n",
    "    tags_sparse = csr_matrix(tags_weighted)\n",
    "    score_sparse = csr_matrix(score_weighted)\n",
    "    popularity_sparse = csr_matrix(popularity_weighted)\n",
    "    \n",
    "    combined = hstack([synopsis_weighted, genres_sparse, tags_sparse, score_sparse, popularity_sparse])\n",
    "    combined = normalize(combined)\n",
    "    return combined\n",
    "\n",
    "def build_user_profile(combined_vectors, matched_indices):\n",
    "    \"\"\"\n",
    "    Builds a user profile vector by averaging the combined feature vectors\n",
    "    of the anime that the user has watched.\n",
    "    \"\"\"\n",
    "    user_vectors = []\n",
    "    for title, idx in matched_indices.items():\n",
    "        user_vectors.append(combined_vectors[idx])\n",
    "    if user_vectors:\n",
    "        user_profile = vstack(user_vectors)\n",
    "        user_profile = user_profile.mean(axis=0)\n",
    "        user_profile = normalize(user_profile)\n",
    "        return user_profile\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def recommend_anime(user_profile, combined_vectors, df, watched_indices, top_n=10):\n",
    "    \"\"\"\n",
    "    Recommends anime based on cosine similarity between the user profile\n",
    "    and the anime vectors in the dataset. Excludes anime already watched.\n",
    "    Returns the top N recommendations with titles and similarity scores.\n",
    "    \"\"\"\n",
    "    similarities = combined_vectors.dot(user_profile.T).toarray().flatten()\n",
    "    recommendations = []\n",
    "    for idx, sim in enumerate(similarities):\n",
    "        if idx in watched_indices:\n",
    "            continue\n",
    "        recommendations.append((idx, sim))\n",
    "    recommendations = sorted(recommendations, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    results = []\n",
    "    for idx, sim in recommendations:\n",
    "        title = df.loc[idx, 'title_romaji'] or df.loc[idx, 'title_english'] or df.loc[idx, 'title_native']\n",
    "        results.append({'title': title, 'similarity': sim})\n",
    "    return results\n",
    "\n",
    "def main(xml_file, csv_file, top_n=10):\n",
    "    # Parse user watchlist from XML (only those with watched episodes > 0)\n",
    "    watchlist = parse_watchlist(xml_file)\n",
    "    print(f\"User watchlist (episodes watched > 0): {watchlist}\")\n",
    "    \n",
    "    # Load the Kaggle anime dataset\n",
    "    df = load_anime_dataset(csv_file)\n",
    "    \n",
    "    # Fuzzy matching: map watchlist titles to dataset indices\n",
    "    matched_indices = match_watchlist_titles(watchlist, df)\n",
    "    watched_indices = list(matched_indices.values())\n",
    "    \n",
    "    # Process features from the dataset\n",
    "    tfidf_matrix, _ = process_description(df)\n",
    "    genres_encoded, _ = process_genres(df)\n",
    "    tags_encoded, all_tags = process_tags(df)\n",
    "    score_scaled, _ = process_numeric_feature(df, 'averageScore')\n",
    "    popularity_scaled, _ = process_numeric_feature(df, 'popularity')\n",
    "    \n",
    "    # Combine and normalize all features into a single vector per anime\n",
    "    combined_vectors = combine_features(tfidf_matrix, genres_encoded, tags_encoded, score_scaled, popularity_scaled)\n",
    "    \n",
    "    # Build the user profile vector based on watched anime\n",
    "    user_profile = build_user_profile(combined_vectors, matched_indices)\n",
    "    if user_profile is None:\n",
    "        print(\"No watched anime found in dataset. Cannot build user profile.\")\n",
    "        return\n",
    "         \n",
    "    # Get recommendations based on cosine similarity\n",
    "    recommendations = recommend_anime(user_profile, combined_vectors, df, watched_indices, top_n=top_n)\n",
    "    \n",
    "    print(\"\\nTop Recommendations:\")\n",
    "    for rec in recommendations:\n",
    "        print(f\"Title: {rec['title']}, Similarity Score: {rec['similarity']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) < 3:\n",
    "        print(\"Usage: python script.py <watchlist_xml_file> <anime_dataset_csv_file> [top_n]\")\n",
    "    else:\n",
    "        xml_file = sys.argv[1]\n",
    "        csv_file = sys.argv[2]\n",
    "        top_n = int(sys.argv[3]) if len(sys.argv) > 3 else 10\n",
    "        main(xml_file, csv_file, top_n)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
